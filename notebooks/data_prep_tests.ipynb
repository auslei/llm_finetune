{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b3eb0f9-628b-4ad9-97f6-41702fdb8bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_templates\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_chat_template\n\u001b[32m      3\u001b[39m tokenizer = get_chat_template(\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mtokenizer\u001b[49m,\n\u001b[32m      5\u001b[39m     chat_template = \u001b[33m\"\u001b[39m\u001b[33mphi-3\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m# Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\u001b[39;00m\n\u001b[32m      6\u001b[39m     mapping = {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33mfrom\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33mgpt\u001b[39m\u001b[33m\"\u001b[39m}, \u001b[38;5;66;03m# ShareGPT style\u001b[39;00m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformatting_prompts_func\u001b[39m(examples):\n\u001b[32m     10\u001b[39m     convos = examples[\u001b[33m\"\u001b[39m\u001b[33mconversations\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"phi-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ff49e9a-f28d-481b-a3d3-e2f3beec651e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0972c7be25c49b6917d724e8eb6187c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\", \n",
    "    data_files=\"./data/anthony/instruct/train.jsonl\", \n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b097530-106e-418e-82b6-4485de2fb216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'Can you explain what Watson Studio is used for in this context?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"In this context, Watson Studio is the business owner's analytics platform. It provides ongoing direction for developing data industrialization functions using both R and Python. This includes tasks from data preparation to monitoring and calibration, aligning with BCBS 239 requirements.\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'What techniques are being used in the significant breach reporting project?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'The significant breach reporting project utilizes semi-supervised and classification (specifically logistic regression) techniques to identify risk events and complaints.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'What is the purpose of the customer vulnerability detection program?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'The customer vulnerability detection program aims to detect vulnerable customers. It does this by using Natural Language Processing (NLP) on customers who send or receive abusive content, and then identifies high-risk customer groups using unsupervised clustering. Logistic regression and random forest are used for feature grid search for clustering.',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c327cac7-3e39-4b21-b9f1-e031872c166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.8: Fast Llama patching. Transformers: 4.53.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 SUPER. Num GPUs = 1. Max memory: 11.593 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=\"unsloth/Phi-3.5-mini-instruct\",\n",
    "            max_seq_length=2048,\n",
    "            dtype=torch.float16,\n",
    "            load_in_4bit=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "672a78a3-be6f-4854-a87a-b6a8612cdba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    all_convos = examples[\"conversations\"]  # List of conversations in the batch\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            convo, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        for convo in all_convos\n",
    "    ]\n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18fae23e-d2a2-4207-b6cc-3e35878fbcce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9247ab7ba521493ea3c7d3952de26b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "dataset = dataset.remove_columns(\"conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c3361e7-6a73-4304-9a9b-eeca77646452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"<|user|>\\nCan you explain what Watson Studio is used for in this context?<|end|>\\n<|assistant|>\\nIn this context, Watson Studio is the business owner's analytics platform. It provides ongoing direction for developing data industrialization functions using both R and Python. This includes tasks from data preparation to monitoring and calibration, aligning with BCBS 239 requirements.<|end|>\\n<|user|>\\nWhat techniques are being used in the significant breach reporting project?<|end|>\\n<|assistant|>\\nThe significant breach reporting project utilizes semi-supervised and classification (specifically logistic regression) techniques to identify risk events and complaints.<|end|>\\n<|user|>\\nWhat is the purpose of the customer vulnerability detection program?<|end|>\\n<|assistant|>\\nThe customer vulnerability detection program aims to detect vulnerable customers. It does this by using Natural Language Processing (NLP) on customers who send or receive abusive content, and then identifies high-risk customer groups using unsupervised clustering. Logistic regression and random forest are used for feature grid search for clustering.<|end|>\\n<|endoftext|>\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98718e26-55b9-4749-9869-24d65c433ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"text\" in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "986583d9-1cc6-455f-909f-614173a804ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"text\" in dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00c795d-d51e-47d0-b2d4-6a352c0dd047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
