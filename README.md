# ğŸ§  LLM Finetune Framework (Unsloth)

This repository provides a complete pipeline for fine-tuning and continually pretraining local LLMs (e.g., Mistral or LLaMA 3.2) using [Unsloth](https://github.com/unslothai/unsloth). It supports both instruction-style fine-tuning and continual pretraining using synthetic or domain-specific data.

---

## ğŸš€ Features

- âš¡ï¸ Fast finetuning and inference using Unsloth's patched models
- ğŸ§¾ Continual pretraining from scratch using `.txt` or `.jsonl`
- ğŸ¤– Instruction fine-tuning with multi-turn chat format
- ğŸ—‚ Modular pipeline (CLI): `scripts/prepare_data.py`, `scripts/finetune.py`, `scripts/chat_interface.py`, `scripts/finetune_shiji.py`
- ğŸ–¥ GUI interface via Streamlit: `app.py`
- ğŸ§ª Test your custom model via terminal chatbot interface
- âœ… Sample finetunes: Pirate Instruct, Zarnian Lore, CV domain expertise


## ğŸ“‚ Project Structure

```
. (root)
â”œâ”€â”€ app.py                     # Streamlit GUI for data-prep, training & chat
â”œâ”€â”€ scripts/                  # CLI entrypoints
â”‚   â”œâ”€â”€ prepare_data.py       # Chunk & prepare pretrain/instruct data
â”‚   â”œâ”€â”€ finetune.py           # Supervised fine-tuning via SFTTrainer
â”‚   â”œâ”€â”€ chat_interface.py     # Terminal-based chat interface
â”‚   â””â”€â”€ finetune_shiji.py     # Shiji dataset example pipeline
â”œâ”€â”€ src/                      # Importable Python package
â”‚   â””â”€â”€ llm_finetune/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ data_prep_tools.py
â”‚       â””â”€â”€ finetune_tool.py
â”œâ”€â”€ notebooks/                # Jupyter notebooks
â”œâ”€â”€ data/                     # Prepared JSONL datasets
â”œâ”€â”€ docs/                     # Raw docs (.txt/.pdf) and guides
â”œâ”€â”€ models/                   # Saved LoRA weights
â”œâ”€â”€ outputs/                  # Checkpoints, logs (git-ignored)
â”œâ”€â”€ llama.cpp/                # llama.cpp artifacts (git-ignored)
â”œâ”€â”€ mac/                      # macOS build artifacts (git-ignored)
â””â”€â”€ unsloth_compiled_cache/   # Cache directory (git-ignored)
```

## ğŸš€ Getting Started

### 1. Install Dependencies
```bash
conda env create -f environment_core.yml
conda activate unsloth3.11
# or with pip: pip install -e .
```

### 2. Run the Streamlit GUI
```bash
export PYTHONPATH=src:$PYTHONPATH
streamlit run app.py
```

### 3. Use the CLI
```bash
# 3.1 Prepare data
python scripts/prepare_data.py \
  --input_file docs/zarnian_lore.txt \
  --output_dir data/Zarnian \
  --mode pretrain

# 3.2 Fine-tune model
python scripts/finetune.py \
  --training_data_path data/Zarnian/pretrain.jsonl \
  --model_name Zarnian \
  --base_model unsloth/Llama-3.2-3B-Instruct-bnb-4bit \
  --mode pretrain \
  --epochs 3

# 3.3 Chat in terminal
python scripts/chat_interface.py \
  --model_dir models/Zarnian

# 3.4 Shiji example
python scripts/finetune_shiji.py
```

## ğŸ“„ License
This project is under the MIT License.

## ğŸ™ Credits
- Unsloth
- HuggingFace Transformers
- Anthony Sun